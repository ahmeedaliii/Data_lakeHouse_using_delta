{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79036c6b-3794-4112-a943-66dc345f68df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: delta-spark in /home/itversity/.local/lib/python3.6/site-packages (1.2.1)\n",
      "Collecting pyspark<3.3.0,>=3.2.0\n",
      "  Using cached pyspark-3.2.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied: importlib-metadata>=1.0.0 in /home/itversity/.local/lib/python3.6/site-packages (from delta-spark) (4.8.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/itversity/.local/lib/python3.6/site-packages (from importlib-metadata>=1.0.0->delta-spark) (4.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/itversity/.local/lib/python3.6/site-packages (from importlib-metadata>=1.0.0->delta-spark) (3.6.0)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /home/itversity/.local/lib/python3.6/site-packages (from pyspark<3.3.0,>=3.2.0->delta-spark) (0.10.9.5)\n",
      "Installing collected packages: pyspark\n",
      "Successfully installed pyspark-3.2.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install delta-spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9602c5b2-7296-4734-9389-5ccd0d5ac064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pyspark\n",
      "Version: 3.1.2\n",
      "Summary: Apache Spark Python API\n",
      "Home-page: https://github.com/apache/spark/tree/master/python\n",
      "Author: Spark Developers\n",
      "Author-email: dev@spark.apache.org\n",
      "License: http://www.apache.org/licenses/LICENSE-2.0\n",
      "Location: /opt/spark-3.1.2-bin-hadoop3.2/python\n",
      "Requires: py4j\n",
      "Required-by: delta-spark\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53362ec4-0032-4f5b-895a-ae3d97189954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/spark2/bin/pyspark: line 45: python: command not found\n",
      "Python 3.6.9 (default, Dec  8 2021, 21:08:43) \n",
      "[GCC 8.4.0] on linux\n",
      "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
      ":: loading settings :: url = jar:file:/opt/spark-3.1.2-bin-hadoop3.2/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/itversity/.ivy2/cache\n",
      "The jars for the packages stored in: /home/itversity/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0146762b-b61d-4418-8009-7c35c252eb15;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;1.0.0 in central\n",
      "\tfound org.antlr#antlr4;4.7 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.7 in central\n",
      "\tfound org.antlr#antlr-runtime;3.5.2 in central\n",
      "\tfound org.antlr#ST4;4.0.8 in central\n",
      "\tfound org.abego.treelayout#org.abego.treelayout.core;1.0.3 in central\n",
      "\tfound org.glassfish#javax.json;1.0.4 in central\n",
      "\tfound com.ibm.icu#icu4j;58.2 in central\n",
      ":: resolution report :: resolve 570ms :: artifacts dl 18ms\n",
      "\t:: modules in use:\n",
      "\tcom.ibm.icu#icu4j;58.2 from central in [default]\n",
      "\tio.delta#delta-core_2.12;1.0.0 from central in [default]\n",
      "\torg.abego.treelayout#org.abego.treelayout.core;1.0.3 from central in [default]\n",
      "\torg.antlr#ST4;4.0.8 from central in [default]\n",
      "\torg.antlr#antlr-runtime;3.5.2 from central in [default]\n",
      "\torg.antlr#antlr4;4.7 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.7 from central in [default]\n",
      "\torg.glassfish#javax.json;1.0.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   8   |   0   |   0   |   0   ||   8   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0146762b-b61d-4418-8009-7c35c252eb15\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 8 already retrieved (0kB/27ms)\n",
      "2024-07-19 12:34:52,774 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2024-07-19 12:35:00,898 WARN yarn.Client: Same path resource file:///home/itversity/.ivy2/jars/io.delta_delta-core_2.12-1.0.0.jar added multiple times to distributed cache.\n",
      "2024-07-19 12:35:00,898 WARN yarn.Client: Same path resource file:///home/itversity/.ivy2/jars/org.antlr_antlr4-4.7.jar added multiple times to distributed cache.\n",
      "2024-07-19 12:35:00,898 WARN yarn.Client: Same path resource file:///home/itversity/.ivy2/jars/org.antlr_antlr4-runtime-4.7.jar added multiple times to distributed cache.\n",
      "2024-07-19 12:35:00,899 WARN yarn.Client: Same path resource file:///home/itversity/.ivy2/jars/org.antlr_antlr-runtime-3.5.2.jar added multiple times to distributed cache.\n",
      "2024-07-19 12:35:00,899 WARN yarn.Client: Same path resource file:///home/itversity/.ivy2/jars/org.antlr_ST4-4.0.8.jar added multiple times to distributed cache.\n",
      "2024-07-19 12:35:00,899 WARN yarn.Client: Same path resource file:///home/itversity/.ivy2/jars/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar added multiple times to distributed cache.\n",
      "2024-07-19 12:35:00,899 WARN yarn.Client: Same path resource file:///home/itversity/.ivy2/jars/org.glassfish_javax.json-1.0.4.jar added multiple times to distributed cache.\n",
      "2024-07-19 12:35:00,899 WARN yarn.Client: Same path resource file:///home/itversity/.ivy2/jars/com.ibm.icu_icu4j-58.2.jar added multiple times to distributed cache.\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.1.2\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.9 (default, Dec  8 2021 21:08:43)\n",
      "Spark context Web UI available at http://itvdelab:4040\n",
      "Spark context available as 'sc' (master = yarn, app id = application_1721387194710_0012).\n",
      "SparkSession available as 'spark'.\n",
      ">>> "
     ]
    }
   ],
   "source": [
    "!pyspark --packages io.delta:delta-core_2.12:1.0.0 \\\n",
    "  --conf \"spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension\" \\\n",
    "  --conf \"spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14f051df-626d-4070-b363-8776c2be2a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1428df93-718f-4835-a917-1c278cfe6726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Spark session\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaExample\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.0.0\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e9eb3603-8c07-4b3c-83e0-5758a9e82bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample DataFrame\n",
    "data = [(\"Alice\", 1), (\"Bob\", 2), (\"Cathy\", 3)]\n",
    "df = spark.createDataFrame(data, [\"name\", \"id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "242b2aec-c38e-4347-a262-c13210078965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame as a Delta table\n",
    "df.write.format(\"delta\").save(\"/delta-final11\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef5382b1-78e7-4896-94fa-3ae5fce0f7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name| id|\n",
      "+-----+---+\n",
      "|Alice|  1|\n",
      "|  Bob|  2|\n",
      "|Cathy|  3|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_read = spark.read.format(\"delta\").load(\"/delta-final11\")\n",
    "df_read.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "031e9787-7bcd-4ee0-a6f8-23bcac975850",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable = DeltaTable.forPath(spark, '/delta-final11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "69f75ee8-0ad2-4acf-8070-0d2351ed5b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name| id|\n",
      "+-----+---+\n",
      "|Alice|  1|\n",
      "|  Bob|  2|\n",
      "|Cathy|  3|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deltaTable.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a79878e-8200-46ba-ae0a-87c71d6e2f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample DataFrame\n",
    "data = [(\"Ahmed\", 1), (\"Bob\", 2), (\"Cathy\", 3), (\"mooo\", 4)]\n",
    "df_new = spark.createDataFrame(data, [\"name\", \"id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a161fd3b-ae3f-45a4-b85e-c02a02233cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we merge (upsert) our target with the source \n",
    "\n",
    "deltaTable.alias(\"target\").merge(\n",
    "    df_new.alias(\"source\"),\n",
    "    \"target.id = source.id\").whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46e01eb6-8000-4958-848f-38a864b6d411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name| id|\n",
      "+-----+---+\n",
      "|Cathy|  3|\n",
      "|Ahmed|  1|\n",
      "| mooo|  4|\n",
      "|  Bob|  2|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deltaTable.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f9653fc2-0d19-45df-80fa-d552ea82abd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th></tr>\n",
       "<tr><td>2</td><td>2024-07-19 16:06:...</td><td>null</td><td>null</td><td>WRITE</td><td>{mode -&gt; Overwrit...</td><td>null</td><td>null</td><td>null</td><td>1</td><td>null</td><td>false</td><td>{numFiles -&gt; 1, n...</td><td>null</td></tr>\n",
       "<tr><td>1</td><td>2024-07-19 13:07:...</td><td>null</td><td>null</td><td>MERGE</td><td>{predicate -&gt; (ta...</td><td>null</td><td>null</td><td>null</td><td>0</td><td>null</td><td>false</td><td>{numTargetRowsCop...</td><td>null</td></tr>\n",
       "<tr><td>0</td><td>2024-07-19 13:04:...</td><td>null</td><td>null</td><td>WRITE</td><td>{mode -&gt; ErrorIfE...</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>true</td><td>{numFiles -&gt; 2, n...</td><td>null</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+\n",
       "|version|           timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|\n",
       "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+\n",
       "|      2|2024-07-19 16:06:...|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|          1|          null|        false|{numFiles -> 1, n...|        null|\n",
       "|      1|2024-07-19 13:07:...|  null|    null|    MERGE|{predicate -> (ta...|null|    null|     null|          0|          null|        false|{numTargetRowsCop...|        null|\n",
       "|      0|2024-07-19 13:04:...|  null|    null|    WRITE|{mode -> ErrorIfE...|null|    null|     null|       null|          null|         true|{numFiles -> 2, n...|        null|\n",
       "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you could review the history of the operations you did and vesions of your data\n",
    "deltaTable.history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9dbafc47-5571-47c6-adff-be8dd59b545d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name| id|\n",
      "+-----+---+\n",
      "|Alice|  1|\n",
      "|  Bob|  2|\n",
      "|Cathy|  3|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read from version 0 of my data\n",
    "\n",
    "df_read = spark.read.format(\"delta\").option(\"versionAsof\",0).load(\"/delta-final11\")\n",
    "df_read.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e1d94aea-fc30-4a94-bcf5-bf4bc5ed6970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name| id|\n",
      "+-----+---+\n",
      "|Cathy|  3|\n",
      "|Ahmed|  1|\n",
      "| mooo|  4|\n",
      "|  Bob|  2|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read from version 1 of my data\n",
    "\n",
    "df_read = spark.read.format(\"delta\").option(\"versionAsof\",1).load(\"/delta-final11\")\n",
    "df_read.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2eac02-96eb-4388-9c6d-6e53672a1d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#deltaTable.optimize().executeCompaction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2419e309-c896-44f6-b21b-72a91c7ad91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "72c7b331-d6c7-4b18-8c62-0d8b70978b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compact all parquet files into one file\n",
    "\n",
    "path = \"/delta-final11\"\n",
    "numFiles = 1\n",
    "\n",
    "(\n",
    "    spark.read.format(\"delta\")\n",
    "    .load(path)\n",
    "    .repartition(numFiles)\n",
    "    .write.option(\"dataChange\", \"false\")\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(path)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "900a3b88-aeef-4207-85a6-3791bc06986a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name| id|\n",
      "+-----+---+\n",
      "|  Bob|  2|\n",
      "|Cathy|  3|\n",
      "|Ahmed|  1|\n",
      "| mooo|  4|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read from the new compact file to make sure the data is saved in one file\n",
    "\n",
    "fi =spark.read.parquet(\"/delta-final11/part-00000-881fe57e-ed6b-48ed-8405-630987b3c5f8-c000.snappy.parquet\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8a9e00-1037-4b35-994f-a71020cbc358",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
